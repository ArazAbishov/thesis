\chapter{Performance evaluation}
In this chapter, I will introduce a methodology for performance evaluation of \rrbvec{}, \pvec{} and their variants, in comparison to implementations from \imrsvec{} and Rust's standard library. We will look at the details of these three stages:

\begin{itemize}
    \item First, a methodology for collecting reliable measurements. 
    \item Then identifying directions for performance comparisons. 
    \item Finally, defining benchmarks. 
\end{itemize}

To begin with, I will present a notion of benchmarking framework, before diving into details of specific profiling tests. 

\section{Methodology}
Big O is a useful tool for reasoning about scalability and performance in theory, and by design, it does not consider factors such as execution environments. First, it disregards constant factors as they are not significant for the growth rate of functions. Second, it does not consider the architecture of CPU and memory \cite{what-programmer-should-know-about-memory}, which indeed influences performance. Furthermore, it also applies to software, such as operating systems, schedulers, virtual machines, et cetera. Hence, often algorithms which are expected to be equally fast based on \bigochar{}, may differ substantially in performance when evaluated experimentally.

This leads to a need for an experimental performance analysis approach, which would involve executing tests on the actual hardware and software. This, however, introduces another set of unique challenges. For instance, depending on the workload, operating systems may allocate more resources for high demanding tasks, by reducing runtime for others \cite{statistically-rigorous-java-performance-evaluation}. Such non-deterministic behavior may lead to profiling results that vary from run to run significantly. 

Benchmarking frameworks were introduced to solve this problem. They are designed to get stable measurements by executing the same test thousands of times. Some of them, such as criterion for Haskell\footnote{\url{https://hackage.haskell.org/package/criterion}} and Rust\footnote{\url{https://crates.io/crates/criterion}}, JMH for Java\footnote{\url{https://openjdk.java.net/projects/code-tools/jmh/}}, and ScalaMeter for Scala\footnote{\url{https://scalameter.github.io/}}, introduce statistical methods for the detection and elimination of exceptionally different runs, known as outliers. 

\subsection{Benchmarking frameworks for Rust}
There are several benchmarking frameworks available for Rust, and unfortunately, none of them reached a stable release yet. However, some of them are being actively used in the Rust community and proven to produce reliable results.  

There are several criteria which a suitable framework has to meet:
\begin{itemize}
    \item Collecting multiple samples where each sample consists of multiple runs to ensure consistent results. 
    \item Detection and elimination of outliers.     
    \item A way for setting up a benchmark before each run. 
    \item A way for preventing compiler optimizing benchmark code away.     
    \item An option to measure execution time without \emph{drop}\footnote{Used to run some code when a value goes out of scope. In other languages, it is sometimes referred to as a destructor.}.
\end{itemize}

\subsubsection*{Rust's benchmark tests}
Rust's testing framework provides an experimental feature\footnote{At the moment of writing, benchmarks are available only in the nightly build of Rust.} that enables developers to write test benchmarks. Those benchmarks are executed thousands of times until results are stabilized. Also, it provides a black-box function\footnote{Black box function contains inline assembly instructions, which compiler cannot make any assumptions about. Hence, it prevents the compiler from optimizing the code which otherwise would be considered "dead" or unused.} which is opaque for the compiler. 

However, it does not detect and eliminate anomalies. It also does not provide APIs for setup routines, which makes it impossible to create benchmarks that rely on certain preconditions.

\subsubsection*{Criterion for Rust}
Criterion for Rust is a powerful and statistically rigorous tool for profiling code. It features outlier elimination, setup routines, and is capable of generating graphs using gnuplot\footnote{\url{http://www.gnuplot.info/}}. At the moment of writing, it is the only framework that has an option to avoid timing of \emph{drop}. 

It is compatible with the stable release of the Rust compiler. Thus, Criterion was chosen as a benchmarking framework for this project. 

\subsubsection*{Alternatives}
Other less popular frameworks, such as \emph{bencher}\footnote{\url{https://crates.io/crates/bencher}} and \emph{easybench}\footnote{\url{https://crates.io/crates/easybench}}, were not considered due to the lack of features necessary for the experiment, such as setup routines, optional measurement of \emph{drop} outlier detection, et cetera. 

\subsection{Configuration and input size}
All tree-based vector implementations, such as \rbvec{}, \rrbvec{}, \pvec{}, and \imrsvec{}, were configured to use a branching factor of 32, as it provides the best tradeoff for the performance of both read and write operations \cite{efficient-immutable-vectors}. 

Each benchmark was executed against a range of input arguments. The input range is specified on the case by case basis depending on the benchmark. 

For each input argument, criterion captures 100 samples. The count of runs per sample is determined dynamically by the library to achieve optimal execution time. 

Benchmarks of the core operations are executed sequentially on a single thread. 

\subsection{Execution environment}
All benchmarks were executed on a computer with an octa-core processor with hyper-threading support, 32GB of DDR4 RAM and 1TB solid-state drive. The operating system is macOS Catalina 10.15.2 with the stable Rust compiler version 1.38.0.

\begin{table}[!htbp]
    \centering

    \begin{tabular} { |l| p{11cm} | }        
        \hline CPU & 2,3 GHz 8-Core Intel Core i9, 16 threads. \\ \hline
        RAM & 32GB DDR4, 2400MHz. \\ \hline
        Disk & 1TB SSD. \\ \hline
        OS & macOS Catalina v10.15.2. \\ \hline     
        Rust & v1.38.0. \\ \hline
        Criterion & v0.3. \\ \hline
        im-rs & v14.0.0. \\ \hline
    \end{tabular}
    
    \label{tab:exec-environment}
    \caption{Hardware and software specification used for benchmarking.}
\end{table}

\section{Benchmarking directions}
To understand how effective proposed optimizations are, we need to evaluate various tree-based vectors, such as \rbvec{}, \rrbvec{}, and \pvec{}. Implementations from \imrsvec{} and the standard library will be evaluated as well. All vector variants are specified in table \ref{tab:vec-implementations} below. 

\begin{table}[!htbp]
    \centering
    
    \begin{tabular} { |l| p{11cm} | }
        \hline
        \stdvec{} & A vector from the Rust's standard library. \\ \hline
        \rbvec{} & \rbtree{} based vector. \\ \hline
        \rrbvec{} & \rrbtree{} based vector. \\ \hline
        \pvec{} & \rrbtree{} based vector with dynamic internal representation. \\ \hline
        \imrsvec{} & \rrbtree{} based vector from the third party library \emph{im-rs}\footnotemark{}. \\ \hline
    \end{tabular}
    
    \label{tab:vec-implementations}
    \caption{A table of vector implementations.}
\end{table}

\footnotetext{Both \imrsvec{} and \pvec{} use \rrbtree{} at its core. It has been developed independently in parallel to \pvec{} at the time of writing this paper: \url{https://crates.io/crates/im}.}

The benchmarks described below are categorized into two groups:
\begin{itemize}
    \item Sequential benchmarks for core operations executed on a single thread. They will be executed both against \rc{} and \arc{} variants of the vector. 
    \item Parallel benchmarks that are executed against \arc{} based vector only. The goal is to verify whether there are benefits of fast split and combine operations of \rrbvec{}.
\end{itemize}

\subsection{Balanced and relaxed vectors}
As an instance of \rrbtree{} is not perfectly balanced and involves the use of size tables for the radix search, it is expected to be somewhat slower in all core operations. This, however, is not true from the perspective of asymptotic analysis, where constant factors are neglected. The goal of the benchmarks, in this case, is to reveal the overhead induced by the relaxed nodes. 

Before each benchmark run, an instance of \rrbvec{} will be prepared by concatenating small vectors together. The count of the relaxed nodes is partially affected by the size of the vector. 

\subsection{Unique access or transience}
While \rrbvec{} performs very well as a persistent data structure, it is not very optimal when persistence is not required. An example is a function that creates and returns an instance of \rrbvec{}, where all versions except the returned one are disregarded.

Luckily, the persistent vector presented in this project takes advantage of Rust's compiler capabilities of tracking object aliasing. Thus, it avoids redundant copying on mutation if the given object is uniquely accessed. This behavior is similar to transience in the Clojure's persistent vector, but not entirely identical \cite{improving-performance-through-transience}. 

In Rust, non-transient, persistent behavior can be enforced by cloning the object before performing a mutation. The objective is to measure the cost of using the clone operation in the persistent vector. 

\subsection{Dynamic internal representation}
As one of the suggested optimizations in the paper on \rrbvec{} implementation for Scala \cite{rrb-vector-practical-general-purpose-im-sequence}, a standard vector can be used to improve the performance of small-sized \rrbvec{}. The size for using the standard vector representation is 1024 for the branching factor of 32, and 256 for the branching factor of 4. However, dynamically switching representation during runtime comes at a cost, which potentially may offset the benefits.

The purpose of profiling this optimization is to understand whether it improves performance in practice, and in which use cases. The problem size range will include small values as well. 

\subsection{Rc vs Arc}
Since atomic reference-counted pointers are claimed to introduce additional overhead in comparison to their non-threadsafe counterpart, the goal is to check how significant is the difference. 

As a part of all sequential benchmarks, both \rc{} and \arc{} variants will be evaluated. \rc{} based flavor will not be present in parallel benchmarks, as the Rust's compiler forbids non-threadsafe types to be used across threads. 

\section{Core operations}
Each benchmark described in this section focuses on a particular core operation of a vector. To avoid ambiguous results, each test exercises only one operation at a time. Operations that modify vector, such as push, will have a complementary version of the benchmark which also uses the clone operation. This is necessary for comparison of the path copying and naive algorithms used in the tree-based and standard vectors correspondingly. 

The following operations were evaluated for vector implementations in \ref{tab:vec-implementations}:
\begin{table}[!htbp]
    \centering

    \begin{tabular} { |l| p{10cm} | }
        \hline 
        Indexing & Accessing vector values. \\ \hline
        Updating & Updating existing values. \\ \hline
        Pushing & Adding new values to the end of a vector. \\ \hline
        Popping & Removing values at the end of a vector. \\ \hline
        Concatenating & Appending values of one vector to another. \\ \hline
        Slicing & Splitting one vector into two at a given position. \\ \hline
    \end{tabular}
    
    \label{tab:vec-core-operations}
    \caption{A table of core operations.}
\end{table}

\paragraph*{Benchmark structure}
Some benchmarks depend on certain preconditions. For example, to test indexing, we first need to create a vector with values. Since building a vector instance should not be measured as a part of that test, it happens in the setup routine. Hence, benchmarks with preconditions are executed in two steps: setup and the actual test. 

\paragraph*{Benchmarking dimensions}
Every benchmark for a core operation is parameterized over the vector size. By providing different arguments, we can observe how the performance of vectors is affected in response. This is especially insightful for the tree-based implementations, where the size of the vector influences the height of the tree, which has a negative impact on performance. The output of a benchmark for a given size is the mean runtime in \ms{}. 

\subsection{Indexing}
In this section, we will define benchmarks for accessing values in three different settings, which model the most common patterns of working with vector: 

\begin{itemize}
    \item Sequentially accessing values by:    
    \begin{itemize}
        \item The index operation which accepts a position as an argument. 
        \item The use of iterator. 
    \end{itemize}
    \item Accessing values at randomly generated positions using the index operation. 
\end{itemize}

In addition to the specific goal, use cases listed above address two objectives: first, how much overhead relaxed nodes of \rrbtree{} introduce in comparison to \rbtree{}, and second, the efficiency of the dynamic internal representation in \pvec{} in comparison to \stdvec{}.

The benchmarks share the same setup routine, which is responsible for generating a vector with values. To generate balanced and relaxed variants of \rbtree{}-based vectors, the setup routine follows two different approaches. \rbvec{}, or balanced \rbtree{}-based vector, is created by simply pushing 64-bit integer values into it. On the other hand, the relaxed variants, such as \rrbvec{} and \pvec{}, are generated by concatenating multiple small instances of vectors together.

The size of the generated vector is determined by the problem size passed as a benchmark argument. The problem size domain in this test is \range{[10, 1m]}. 

\subsubsection*{Index sequentially and iterating}
In the benchmark variant with access by index, the test function loops over the array of \range{[0, N)} indices, reading values from a vector at corresponding positions. \n{} is the problem sizes passed to the benchmark as an argument. It is important to emphasize, that in this benchmark, values are read through immutable, read-only references. In Rust, it means that values are not moved and still, belong to a vector. 

The second benchmark, based on iterators, reads the contents of the vector from the beginning to the end, without accessing values by index. By leveraging this knowledge, the iterator implementations of \rbvec{}, \rrbvec{}, and \pvec{}, read values from the tree by chunks, rather than by individual values. However, the mechanism of reading values is different compared to the first benchmark. Instead of immutably borrowing values, iterator takes ownership of them. This implies a different memory reclamation behavior, which makes the direct comparison to the first benchmark irrelevant. Hence, the results of this benchmark will be evaluated independently. 

\subsubsection*{Index randomly}
In this benchmark, rather than accessing values consecutively, they will be read at random positions. From the algorithmic point of view, the implementation of the operation itself is the same. As indices are picked randomly, it is quite likely that desired values will be located far apart in memory, which causes a cache invalidation. Additionally, results will show whether the performance degenerates with randomness, as it would with linked lists for example. 

The test function contains a loop, which is executed \n{} times. In the loop body, a value is accessed at random index, which is generated within the \range{[0, N)} range by using the \crate{rand} crate\footnote{A Rust library for random number generation: \url{https://crates.io/crates/rand}}. According to the \crate{rand} documentation, generated indices are uniformly distributed. The number generator is explicitly seeded to produce the same stream of randomness multiple times.

\subsection{Updating}
There are two dimensions in which the update operation will be evaluated. The first one, similar to the index operation, is the order in which vector values are updated: sequential and random. The second dimension introduces the clone operation, which is used in combination with update to reveal how cloning affects performance. 

Here is the resulting list of benchmarks:
\begin{itemize}
    \item Updating a vector instance sequentially:
    \begin{itemize}        
        \item With and without using \emph{clone}.
    \end{itemize}
    \item Updating a vector at random positions:
    \begin{itemize}
        \item With and without using \emph{clone}.
    \end{itemize}  
\end{itemize}

The setup routine for all benchmarks is identical. As for the index benchmarks, it generates both balanced and relaxed variants of the tree-based vectors. The type of generated values is an unsigned 64-bit integer. 

The size of the generated vector is determined by the problem size passed as an argument. The problem size domain for benchmarks using clone is \range{[10, 20000]}, which is smaller compared to the \range{[10, 100000]} range, used for benchmarks without clone. This is done to reduce the runtime of benchmarks. 

\paragraph*{The cost of naive clone vs path copying}
One of the claimed advantages of \rbvec{} over \stdvec{}, is the cheap clone operation enabled by the path copying algorithm of \rbtree{}. However, the naive copy of \stdvec{} is faster for small-sized vectors due to its simplicity and better locality features. \pvec{} takes the best of both worlds, by using \stdvec{} for the size up to 1024 elements, after which it switches to \rbvec{}. Hence, the objectives of the update benchmarks involving the clone operation are:
\begin{itemize}
    \item Compare performance of naive and path copying algorithms. 
    \item Evaluate the efficiency of dynamic internal representation in \pvec{}.  
\end{itemize}

\paragraph*{The overhead of relaxed nodes in \rrbtree{}}
Relaxed nodes of \rrbtree{} use size tables to keep track of the size of its child nodes. Balanced nodes, on the other hand, do not need them, as the size can be derived from the level of the node. Hence, relaxed nodes are more expensive to clone. Additionally, \rrbtree{} is not perfectly balanced as \rbtree{}, potentially resulting in taller trees. The results of benchmarks will reveal how significant this overhead is in practice. 

\subsubsection*{Update sequentially}
The test function iterates over indices in the \range{[0, N)} range, where \n{} is the problem size, acquiring a mutable reference to the value at the given position. Once the reference is acquired, it is used to increment the value. 

\subsubsection*{Update randomly}
The test function contains a loop, which is executed \n{} times. In the loop body, value is updated by incrementing it, at the index that is randomly generated in the \range{[0, N)} range. 

\subsubsection*{Extending benchmarks with the clone operation}
Both benchmarks are extended with the clone operation. The test variant with clone introduces an additional variable for keeping track of the cloned vector. This is done to ensure that at least two vector instances exist at the time when the update is executed. This is necessary because \rc{} pointers used to implement \rbtree{}, clone the underlying value on mutation only when the reference count is bigger than one. Thus, by having a cloned instance of vector present in the scope, we enforce the path copying algorithm to be used when updating a vector. 

\subsection{Pushing}
An operation used to add values at the end of vector is called push. In this benchmark, we will focus on building a vector from the empty state, as well as pushing values into a prebuilt vector of different types. Additionally, the aforementioned benchmarks will be extended with the clone operation to evaluate the efficiency of \pvec{}'s dynamic internal representation:
\begin{itemize}
    \item Building a vector from the empty state.    
    \begin{itemize}
        \item With and without \emph{clone}. 
    \end{itemize}
    \item Pushing values on top of the existing vector.
    \begin{itemize}
        \item With and without \emph{clone}. 
    \end{itemize}
\end{itemize}

\paragraph*{The overhead of relaxed nodes in \rrbtree{}}
The push operation is responsible for increasing the capacity of a vector. While the push operation of \rrbtree{} heavily relies on the size tables for calculating the capacity of nodes, for \rbtree{} it is sufficient to have the level of the node and the branching factor to perform the same calculation. Additionally, constructing new relaxed nodes requires allocating size tables. All these factors combined are expected to make \rrbtree{}'s push slower compared to \rbtree{}. Thus, there is a dedicated benchmark that uses prebuilt, \rrbtree{} and \rbtree{} based vectors to evaluate the difference.  

\paragraph*{Building a vector}
As vector is built from scratch in this benchmark, there is no need for a setup routine. The test function runs a loop over the \range{[0, N)} range of indices, and pushes the index as a value into a vector. The problem size range is \range{[10, 1m]}. 

\paragraph*{Pushing values onto existing vector}
This benchmark requires an existing vector. Hence, the setup routine generates a vector of the fixed size of \n{}, and passes it to the test function. The balanced, \rbtree{} based vector is created by pushing values directly into it, while the \rrbtree{} based one, is created by concatenating several vectors together. Once a vector is created, the test function pushes \n{} values onto it. 

\paragraph*{Extending benchmarks with the clone operation}
To force \pvec{} to switch from \stdvec{} to \rrbvec{}, several preconditions have to be met, including the reference count being bigger than 1. Hence, the benchmarks above are extended to use clone, in the same way as for benchmarks of the update operation. Beyond the evaluation of \pvec{}, the results are expected to reveal how tree-based vectors stack up to \stdvec{}. The input range for benchmarks using clone is \range{[10, 20k]}.

\subsection{Popping}
The pop operation is used to remove values at the end of the vector. As with push, it is responsible for managing the capacity of the underlying data structure. For \stdvec{} it means shrinking the array and copying elements over. For tree-based vectors, it implies deallocating nodes and reducing the height of the tree when necessary. 

The benchmark is divided into two tests, namely pop and pop clone. The first test implies calling pop continuously in the loop until the vector is emptied, with the problem size of \range{[10, 60000]}. 

In the second benchmark, each pop operation will be followed by a clone. Both benchmarks will be executed against relaxed and balanced vector variants, which are prepared in the setup routine. The problem size range is \range{[10, 20000]}.

\paragraph*{The overhead of relaxed nodes in \rrbtree{}}
Lowering the height of \rrbtree{} involves the calculation of tree capacity using size tables, which comes at an additional cost. Thus, one of the goals is to measure and compare the cost of pop for both balanced and relaxed variants of vectors. 

\paragraph*{Dynamic internal representation}
The pop operation is responsible for switching the internal representation of \pvec{} from \rrbvec{} to \stdvec{} when the size gets smaller than 1024. Hence, one of the objectives is to evaluate how is that reflected on performance. 

\subsection{Concatenating}
The concatenation operation merges contents of one vector into another. One of the claimed advantages of \rrbtree{} is the relatively low cost of concatenation operation, that is \bigo{(m^2 \cdot log_m(n)}, in comparison to \bigo{max(a,b)} of \stdvec{}. The objective of this benchmark is to validate performance properties of the concatenation operation experimentally. 

\paragraph*{Naive vs relaxed concatenation algorithm}
The vector that is based on \rbtree{} uses a naive concatenation algorithm, that essentially is moving values from one vector to another. \rrbtree{} on the other hand merges and rebalances two trees, which theoretically is faster. Due to the hardware design specifics, this might not be true for all vector sizes. Thus, benchmarks will reveal how different algorithms perform depending on the size of concatenated vectors. 

\paragraph*{Dynamic internal representation}
Evaluating whether the \stdvec{} optimization used in \pvec{} positively impacts performance during concatenation, as well as the cost of switching to the \rrbvec{} representation. 

\paragraph*{Concatenating vectors}
The setup routine prepares a collection of vectors, where each consecutive vector is bigger than the previous. The total size of all prepared vectors adds up to the problem size. The benchmark is parameterized over the vector size, which will be in the \range{[10, 1m]} range. 

Each vector is created by a combination of concatenation and push operations. This way \pvec{} and \rrbvec{} will be forced to use \rrbtree{} for internal representation, while \rbvec{} will remain balanced. \stdvec{} remains flat and does not depend on the type of operation used to add values to it. 

The benchmark function iterates over generated vectors and concatenates them into a vector defined as a local variable. 

\subsection{Slicing}
The slice operation splits a vector into two parts at the given index. The \rrbtree{}'s algorithm theoretically can achieve good performance by avoiding unnecessary copying. However, due to its complexity, it might be outperformed by naive copying for small-sized vectors.

\paragraph*{Naive vs relaxed slicing algorithm}
This benchmark is aimed to compare \rrbvec{}'s slice algorithm to \stdvec{}'s and \rbvec{}'s naive copying. For \pvec{}, on the other hand, we will evaluate whether using different representations gives any advantage in performance. 

\paragraph*{Slicing vectors}
The test itself anticipates a prepared vector, which is generated in the setup routine. To evaluate both balanced and relaxed variants of vectors, it generates them differently by either using concatenation or simple pushing. The vector sizes are varying in the \range{[128, 40000]}. 

Once a vector is generated, the benchmark function enters the loop with a condition that vector needs to contain more than 64 elements. In the loop, a vector is split at index 64, the result of which is assigned back to a vector variable. Essentially, a vector is being truncated at the front by 64 elements, until it is small enough for the loop to exit. 

\section{Parallel vector}
One of the claims is that \rrbvec{} is very efficient when it comes to split and concatenate operations. The data parallelism frameworks, such as Rayon\footnote{\url{https://crates.io/crates/rayon}}, Cilk\footnote{\url{http://supertech.lcs.mit.edu/cilk/}}, and Scala's parallel collections\footnote{\url{https://docs.scala-lang.org/overviews/parallel-collections/overview.html}}, split the work into smaller chunks to ensure good parallelism. Thus, fast split and concatenation operations are critical for optimal performance. 

In this section, we will first take a look at how Rayon splits and distributes the work across threads, as well as available configuration parameters. Section \ref{sec:par-benchmarks} introduces tests for benchmarking the overall performance of persistent and standard vectors:

\begin{itemize}    
    \item Adding elements of two vectors.    
    \item Check if a word is a palindrome.         
\end{itemize}

All the tests will be executed on 1, 2, 4, 8, and 16 threads. 

Unlike the measurements presented from the sequential benchmarks, the parallel ones include the runtime of both vector operations and Rayon. As the objective is the overall performance comparison, this is considered to be an acceptable tradeoff. 

The results will be used to evaluate the effectiveness of the following optimizations:
\begin{itemize}
    \item The effect of relaxed concatenation and split operations of \rrbvec{} on the overall performance.
    \item Dynamic internal representation of \pvec{}. 
\end{itemize}

\subsection{Rayon}
Rayon, a data parallelism library for Rust, helps to turn sequential code into parallel with as little work as possible. Loops and iterators are often used to process collections sequentially. Rayon, on the other hand, offers a potentially more efficient alternative to them in the form of parallel iterators. It takes advantage of modern processors, by dividing the work between available cores as long as it is beneficial. 

\paragraph*{Parallel iterators}
\begin{figure}[!htbp] 
    \centering

    \begin{minted}{rust}
        // sequential iterator
        vec![1, 2, 3]
            .into_iter()
            .for_each(|x| println!("{}", x));

        // rayon's parallel iterator
        vec![1, 2, 3]
            .into_par_iter()
            .for_each(|x| println!("{}", x));
    \end{minted}

    \caption{Example of using sequential and parallel iterators.}
    \label{fig:par-iter-example}
\end{figure}

The convenience of Rust iterators is in the provided operators that are called \emph{combinators}. Combinators can be chained and combined, allowing a developer to perform complex manipulations of iterators safely and efficiently. 

Parallel iterators provide a similar set of combinators, even though not entirely identical. As iterators process values sequentially, there is a set of combinators that expect values to be emitted in a particular order. As the parallel iterators are designed to process data in any order, inherently sequential combinators are simply not applicable. Thus, Rayon might be not a suitable solution for algorithms relying on the sequential order of execution.

Another limitation which parallel iterators impose, is that type of values which it works with have to implement the \emph{Send} trait. It means using non-threadsafe types such as \rc{} in combination with Rayon is prohibited. 

\paragraph*{Work splitting}
\begin{figure}[!htbp]
    \centering

    \begin{tikzpicture}[
        font=\ttfamily,
        array/.style={
            matrix of nodes,
            nodes={draw, minimum size=7mm, fill=green!30},
            column sep=-\pgflinewidth, 
            row sep=0.5mm, 
            nodes in empty cells,        
            row 1 column 1/.style={nodes={draw}}
        }]
                
        \matrix[array] (array) { 
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
        };            
        
        \draw[|-|]([yshift=-4mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {12} ([yshift=-4mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-6.south east);
        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-7.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-3.south east);
        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-4.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-6.south east);
        \draw[loosely dotted]([yshift=-12mm,xshift=1mm]array-1-7.south west) -- ([yshift=-12mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {2} ([yshift=-16mm,xshift=-1mm]array-1-2.south east);
        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-3.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-16mm,xshift=-1mm]array-1-3.south east);
        \draw[loosely dotted]([yshift=-16mm,xshift=1mm]array-1-4.south west) -- ([yshift=-16mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-1.south east);
        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-2.south east);
        \draw[loosely dotted]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- ([yshift=-20mm,xshift=-1mm]array-1-12.south east);    
        
        \draw ([xshift=1mm]array-1-12.east)--++(0:3mm) node[right]{ Vector };
    \end{tikzpicture}

    \caption{Visualization of work splitting in Rayon.}
    \label{fig:rayon-work-splitting}
\end{figure}

One of the Rayon's components, a fork/join framework, is responsible for dividing and distributing the work between threads. When parallel iterator receives values from a collection like a vector, Rayon attempts to repeatedly divide the work into chunks among threads until the chunk is small enough for a single thread. For an example, see figure \ref{fig:rayon-work-splitting}.

As demonstrated in figure \ref{fig:rayon-join}, the work is \emph{potentially} divided between two threads by calling \mintinline{rust}{rayon::join}, which accepts two closures. Rayon decides whether it is beneficial to parallelize the work, depending on the count of available threads, the split factor, and the workload. If the problem is small enough, it is solved sequentially. Otherwise, it is subdivided into smaller parts. When both closures finish working, the results are combined and returned to the caller. 

The size of the work chunk, or the \emph{split factor}, can be controlled by two operations available for \emph{IndexedParallelIterator}, namely \emph{with\_min\_len} and \emph{with\_max\_len}. The use of combinators which may affect the size of a collection, such as \emph{filter}, returns a \emph{ParallelIterator} which does not support configuration of the \emph{split factor}. 

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        rayon::join(
            || do_something(...),
            || do_something_else(...)
        );
    \end{minted}
    
    \caption{An example for using rayon's join.}
    \label{fig:rayon-join}
\end{figure}

By default, the count of threads allocated by Rayon is equal to the number of cores available in the system. To observe how the thread count affects the performance, Rayon's thread pool will be configured to work with 2, 4, 8, and 16 threads. 

\paragraph*{Load balancing}
In a perfect world, the chunks of work split between threads take the same amount of time to process. In reality, this is often not the case, resulting in some threads idling. In Rayon, each thread has a queue of work attached to it. It keeps processing the queue until it becomes empty. To avoid idling, the thread which has finished processing its queue can steal work from another thread. This technique is known as work-stealing and is used as the main mechanism for work distribution in Rayon. 

\paragraph*{Computation stages}
Computation stages of both "Add elements of two vectors" and "Check if a word is a palindrome" benchmarks, can be described in three steps. 
\begin{itemize}
    \item First, split the work between threads. 
    \item Then process the chunk of work sequentially. 
    \item Finally, combine and return the results.    
\end{itemize}

The final step can be subdivided further:
\begin{itemize}
    \item Collect individual items into a vector using the parallel \emph{Fold} combinator. 
    \item Reduce emitted vectors into a single one using the \emph{Reduce} combinator. 
\end{itemize}

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        let result = parallel_iterator
            .fold(Vec::new, |mut vec, x| {
                vec.push(x);
                vec
            })
            .reduce(Vec::new, |mut vec1, mut vec2| {
                vec1.append(&mut vec2);
                vec1
            });
    \end{minted}
    
    \caption{Collecting items of parallel iterator.}
    \label{fig:fold-reduce}
\end{figure}

\subsection{Benchmarks}
\label{sec:par-benchmarks}
The benchmarks were executed against following vector implementations: \stdvec{}, \rbvec{}, \rrbvec{}, and \pvec{}, where \rbvec{}, \rrbvec{}, and \pvec{} are based on the threadsafe reference-counted pointer -- \arc{}. The \imrsvec{} is not included because it does not implement the Rayon's \emph{IntoParallelIterator} trait, which makes its evaluation irrelevant. 

Benchmarks have been parameterized over two dimensions: the vector size and the number of threads. To see whether parallelism is beneficial, each benchmark has an analogous, sequential implementation executed on a single thread. 

\subsubsection*{Sum of elements of two vectors}
Given two equally sized vectors of integers, the test function adds values at the corresponding indices and returns a new instance of a vector with results. The benchmark is subdivided into three steps:

\begin{enumerate}
    \item Transform each vector into a parallel iterator and merge them into a single sequence of value pairs. 
    \item Add items of the emitted tuple of two integers into a single value.
    \item Reduce individual sums into a vector of results.
\end{enumerate}

The setup routine prepares two vectors of integers the \range{[0, N]} problem size range. 

\subsubsection*{Check if a word is a palindrome}
The benchmark checks whether a word is a palindrome. As input, we are using a list of English words consisting of only alphabetic characters. The benchmark consists of two variants:
\begin{itemize}
    \item Annotating every word with a boolean which indicates if the word is a palindrome. 
    \item Collecting palindromes into a new vector. 
\end{itemize}

The computation stages of both tests are very similar code-wise, with the difference present in the operators used to process each word:

\begin{enumerate}
    \item Transform the given vector of words into a parallel iterator. 
    \item A word processing step for two tests correspondingly:
    \begin{enumerate}
        \item Return a tuple containing the word and the flag which indicates whether the word is a palindrome. 
        \item Filter out all words which are not palindromes. 
    \end{enumerate}
    \item Reduce the results to a new instance of a vector.  
\end{enumerate}

Essentially, the difference comes down to the operators used. For the first test, each word is processed by the \emph{Map} combinator. It takes a string as an argument and returns a tuple of a string and boolean. 

The second test relies on the \emph{Filter} combinator, which takes a predicate as an argument. The predicate, in this case, is the function checking if the word is a palindrome. 

An important difference in behavior between two tests is that filter alters the length of the resulting vector, while the map combinator does not. 

Hence, the difference in performance between standard and persistent vector might become more apparent in the second test, as Rayon will not be able to make optimizations based on the assumptions made about the size of the resulting vector. 

\paragraph*{The benchmark setup}
The total count of words stored in the file is 370103. As the benchmark is parameterized over the size of a vector, which in this case is equal to the number of words, the setup routine caps the count of words by \n{}.

The contents of the file are loaded into memory once, and then before each run, they are copied over to a new vector within the setup routine. A vector is later passed to the test routine that converts it to a parallel iterator. 

\section{Presentation of results}
The measurements are done in a number of samples that can be configured. By default, the count of samples is set to a 100, and this default is being used for this project. Each sample consists of one or typically more iterations of the routine. The elapsed time between the beginning and the end of the iterations, divided by the number of iterations, gives an estimate of the time taken by each iteration. 

As measurement progresses, the sample iteration counts are increased. The presented graphs use the mean measured time for each function. Additionally, benchmarks were configured to avoid timing the execution of \emph{drop}. 

\section{Reproducing results}
Benchmarks can be compiled and executed using Rust's package manager named \emph{cargo}. By default, sequential benchmarks are executed against the non-threadsafe variant of the persistent vector. To select the threadsafe variant, the user can pass the \arc{} feature flag as demonstrated in figure \ref{fig:sequential-benches}. To run a particular benchmark, specify its name as an argument to cargo. 

\floatstyle{boxed}
\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        # benches of the non-threadsafe implementation
        cargo bench

        # benches of the threadsafe implementation
        cargo bench --features=arc 
    \end{minted}
    
    \caption{Example 1.}
    \label{fig:sequential-benches}
\end{figure}

To execute parallel benchmarks, user needs to pass both the \arc{} and \emph{rayon-iter} feature flags: 

\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        cargo bench --features=arc,rayon-iter
    \end{minted}
    
    \caption{Example 2.}
    \label{fig:parallel-benches}
\end{figure}

Criterion can also generate HTML reports with charts using \emph{gnuplot}\footnote{\url{http://www.gnuplot.info/}}. Results can be found in the \emph{pvec-rs/target/criterion} directory, and if \emph{gnuplot} is available, the report will be located at \emph{pvec-rs/target/criterion/report/index.html}. 

For more information on available options and parameters for Criterion, please consult the library documentation\footnote{\url{https://docs.rs/criterion/0.3.0/criterion/}}.
