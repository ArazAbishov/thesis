\chapter{Performance evaluation}

In this chapter, I will introduce a methodology for performance evaluation of \rrbvec{}, \pvec{} and their variants, in comparison to implementations from \imrsvec{} and the Rust's standard library. We will look at the details of these three stages:

\begin{itemize}
    \item First, a methodology for collecting reliable measurements. 
    \item Then identifying directions for performance comparisons. 
    \item Finally, defining benchmarks. 
\end{itemize}

To begin with, I will present a notion of benchmarking framework, before diving into details of specific profiling tests. 

\section{Methodology}
Even though \bigo{} \todo{Use a word representation of Big-O} is a useful tool for reasoning about performance in theory, it often is not accurate enough for evaluating real-world performance. First, it disregards constant factors as they are not significant for the growth rate of functions. Second, it does not consider the architecture of CPU and memory \todo{ref}, which indeed influences performance. Furthermore, it also applies to software, such as operating systems, schedulers, virtual machines, et cetera. Hence, often algorithms which are expected to be equally fast based on \bigo{}, may differ substantially in real-world performance. 

This leads us to a need for an experimental performance analysis approach, which would involve executing tests on the actual hardware and software. This, however, introduces another set of unique challenges. For instance, depending on the workload, operating systems may allocate more resources for high demanding tasks, by reducing runtime for others \todo{ref}. Such non-deterministic behavior may lead to profiling results which vary from run to run significantly, which defeats the purpose of having them. 

Benchmarking frameworks were introduced to solve those problems. They are designed to get stable measurements by executing the same test thousands of times. Some of them, such as criterion \todo{ref} for Haskell and Rust, JMH \todo{ref} for Java, and ScalaMeter for Scala, introduce statistical methods for the detection and elimination of exceptionally different runs, known as outliers. 

\subsection{Benchmarking frameworks for Rust}

There are several benchmarking frameworks available for Rust, and unfortunately, none of them have reached a stable release yet. However, some of them are being actively used in the Rust community and proven to produce reliable results.  

In our case, there are several criteria which a good framework has to meet:

\begin{itemize}
    \item Collects multiple samples where each sample consists of multiple runs to ensure consistent results. 
    \item Detection and elimination of outliers.     
    \item A way for setting up a benchmark before each run. 
    \item A way for preventing compiler optimizing benchmark code away.     
\end{itemize}

\subsubsection*{Rust's benchmark tests}
The Rust's testing framework provides an experimental feature which enables developers to write test benchmarks. Those benchmarks are executed thousands of times until results are stabilized. Also, it provides a black-box function \footnote{Black box function contains inline assembly instructions, which compiler cannot make any assumptions about. Hence, it prevents the compiler from optimizing the code which otherwise would be considered "dead" or unused.} which is opaque for the compiler. 

However, it does not detect and eliminate anomalies. It also does not provide APIs for setup routines, which makes it impossible to create benchmarks which rely on certain preconditions.

% ToDo: consider talking about measurement of time for dropping items
\subsubsection*{Criterion for Rust}
Criterion for Rust \todo{ref}, not to be confused with Criterion for Haskell \todo{ref}, is a powerful and statistically rigorous tool for profiling code. It features outlier elimination, setup routines, and is capable to generate graphs provided that gnuplot is installed \todo{ref}. it is available for the stable Rust compiler. Thus, Criterion was chosen as a benchmarking framework for this project. 

\subsection{Execution environment}
All benchmarks were executed on a computer \todo{table with hardware, software} with a quad-core Intel Core i5-6600 processor with hyperthreading, 16GB of DDR4 RAM and 250GB solid-state drive. The operating system of the choice is Ubuntu 18.04 with nightly Rust compiler version \todo{Rust version}.

\subsection{Configuration and input size}
\todo{section}

\section{Benchmarking directions}
% ToDo: you have completely forgotten about branching factor, and it being a standalone feature
% ToDo: you haven't talked about input size and configuration of benchmarks (what will be the input size). You can steal input numbers from scala paper, and then tweak them to run for reasonable amount of time. 

To understand how effective certain optimizations are, we need to evaluate various configurations of the persistent vector. Additionally, implementations from \imrsvec{} and the standard library will be tested too. All vector variants are specified in table \todo{ref}. 

% ToDo: try to name types after the name of types in actual code, otherwise thins will get very confusing very fast. Also, I don't think it is important to show distinction between Rc / Arc flavors of implementations, because it just introduces more confusion. 
\begin{table}
    \centering

    \begin{tabular} { |l| p{10cm} | }
        \hline
        STD Vec & Vec from the Rust's standard library. \\ \hline
        \rbvec{} & \rbtree{} based vector. \\ \hline
        \rrbvec{} & \rrbtree{} based vector. \\ \hline
        \pvec{} & \rrbtree{} based vector with dynamic internal representation. \\ \hline
        \imrsvec{} & \rrbtree{} based vector from third party library \imrsvec{}. \\ \hline        
    \end{tabular}
    
    \label{tab:vec-implementations}
    \caption{A table of vector implementations.}
\end{table}

% \begin{wip}
    % For the purpose of comparison, the measurements are taken against the standard vector, as well as the \rrbtree based persistent vector implementation from the third party library named \emph{im-rs}, which has been introduced at the time of writing this paper. The persistent vector presented in this work, has been evaluated using both non-atomic and atomic reference counted pointers, as well as with and without small sized vector based optimization. 
% \end{wip}

In general, benchmarks described below could be categorized into two groups:

\begin{itemize}
    \item First, serial benchmarks for profiling core operations in a sequential environment. They will be executed both against threadsafe and non-threadsafe variants of the vector. 
    \item Second, concurrent benchmarks which will be executed against only thread-safe variants of the vector. The goal is to check whether there are benefits of using fast split and combine operations of \rrbvec{}.
\end{itemize}


\subsection{RB and RRB Vectors}
As relaxed balanced tree is not perfectly balanced and involves the use of size tables for the radix search, it is expected to be somewhat slower in all core operations. This, however, is not true from the perspective of asymptotic analysis, where constant factors are neglected. The goal of benchmarks, in this case, is to reveal the overhead induced by relaxed nodes. 

Before each benchmark run, an instance of \rrbvec{} will be prepared by concatenating pseudo-random small vectors together. The amount of relaxed nodes is in part affected by the size of the vector. Both threadsafe and non-threadsafe variants will be compared. 

% ToDo: how to consistenly generate RRB Vectors using pseudo randomly sized small vectors? Can you get access to benchmarks from paper anywhere? 
% ToDo: add list of benchmarks which will be evaluating different core operations. 

\subsection{Unique access or transience}
While \rrbvec{} performs very well as a persistent data structure, it is not very optimal when properties of persistence are not required. An example is a function which creates and returns an instance of \rrbvec{}, where all versions except the returned one are disregarded.

Luckily, the persistent vector presented in this project takes advantage of Rust's compiler capabilities of tracking object aliasing. Thus, it avoids redundant copying on mutation if the given object is uniquely accessed. This behavior is somewhat similar to transience in the Clojure's persistent vector, but not entirely identical \todo{see reference}. 

In Rust, non-transient, persistent behavior can be enforced by cloning the object before performing a mutation. The goal is to measure the overhead of using clone operation in the persistent vector. 

\subsection{Dynamic internal representation}
As one of the suggested optimizations in \todo{ref to scala paper}, a standard vector can be used to improve the performance of small-sized \rrbvec{}. The size recommended for using the standard vector representation is 4096. However, dynamically switching representation during runtime comes at a cost, which potentially may offset the benefits. 

The purpose of profiling this optimization is to understand whether it improves performance in practice, and in which use cases. The range of problem sizes will include small values. 

\todo{describe an experiment; i.e. how many threads, split factor, problem sizes, etc.}
\todo{how number of threads is configured in rayon: 1, 2, 4, 8, 16, 32 and 64}
\todo{which experiments will you run?}

\subsection{Memory overhead}
\todo{tbd}

\subsection{Rc vs Arc}
Since atomic reference-counted pointers are claimed to introduce additional overhead in comparison to their non-threadsafe counterpart, the goal is to check how significant is the difference. 

As a part of all sequential benchmarks, both threadsafe and non-threadsafe variants will be evaluated. Rc based flavor will not be present in parallel benchmarks, as the Rust's compiler prevents non-threadsafe types being used across threads. 

\section{Core operations}
Each benchmark described in this section has a focus on a particular core operation of a vector. To avoid ambiguous results, each test exercises only one operation at a time. Operations that modify vector, such as push, will have a complementary version of the benchmark which also uses the clone operation. This is necessary for comparison of the path copying and naive algorithms used in the tree based and standard vectors correspondingly. 

The following operations were evaluated for vector implementations in \ref{tab:vec-implementations}:

\begin{table}[!htbp]
    \centering

    \begin{tabular} { |l| p{10cm} | }
        \hline 
        Indexing & Accessing vector values. \\ \hline
        Updating & Updating existing values. \\ \hline
        Pushing & Adding new values to the end of a vector. \\ \hline
        Popping & Removing values at the end of a vector. \\ \hline
        Concatenating & Appending values of one vector to another. \\ \hline
        Slicing & Dividing one vector into at a given position. \\ \hline        
    \end{tabular}
    
    \label{tab:vec-core-operations}
    \caption{A table of core operations.}
\end{table}

\paragraph*{Benchmark structure}
Some benchmarks depend on certain preconditions. For example, to test indexing, we first need to create a vector with values. Since building a vector instance should not be measured as a part of that test, it happens in the setup routine. Hence, benchmarks with preconditions are executed in two steps: setup and the actual test. 

\paragraph*{Benchmarking dimensions}
Every benchmark for a core operation is parameterized over the vector size. By providing different arguments, we can observe how the performance of vectors is affected in response. This is especially insightful for the tree based implementations, where the size of the vector influences the height of the tree, which has a negative impact on performance. The output of a benchmark for a given size is the mean run time in \todo{ms}. 

\subsection{Indexing}
In this section, we will define benchmarks for accessing values in three different settings, which model the most common patterns of working with vector: 

\begin{itemize}
    \item Sequentially accessing values by:    
    \begin{itemize}
        \item The index operation which accepts a position as an argument. 
        \item The use of iterator. 
    \end{itemize}
    \item Accessing values at randomly generated positions using the index operation. 
\end{itemize}

In addition to the specific goal, use cases listed above address two objectives: first, how much overhead relaxed nodes of \rrbtree{} introduce in comparison to \rbtree{}, and second, the efficiency of the dynamic internal representation in \pvec{} in comparison to \stdvec{}.

The benchmarks share the same setup routine, which is responsible for generating a vector with values. To generate balanced and relaxed variants of \rbtree{} based vectors, the setup routine follows two different approaches. \rbvec{}, or balanced \rbtree{} based vector, is created by simply pushing integer values \footnote{The type of value is a 64-bit unsigned integer.} into it. On the other hand, the relaxed variants, such as \rrbvec{} and \pvec{}, are generated by concatenating multiple small instances of themselves together.

The size of the generated vector is determined by the problem size passed as a benchmark argument. The problem size domain in this test is [10, 1M]. 

\subsubsection*{Index sequentially and iterating}
In the benchmark variant with access by index, the test function loops over the array of [0, N) indices, reading values from a vector at corresponding positions. N is the problem sizes passed to the benchmark as an argument. It is important to emphasize, that in this benchmark, values are read through immutable, read-only references. In Rust, it means that values are not moved \footnote{todo: see introduction section} and still, belong to a vector. 

The second benchmark, based on iterators, reads the contents of the vector from the beginning to the end, without accessing values by index. By leveraging this knowledge, the iterator implementations of \rbvec{}, \rrbvec{}, and \pvec{}, read values from the tree by chunks, rather than by individual values. However, the mechanism of reading values is different compared to the first benchmark. Instead of immutably borrowing values, iterator takes ownership of them. This implies a different memory reclamation behavior, which makes the direct comparison to the first benchmark irrelevant. Hence, the results of this benchmark will be evaluated independently. 

\subsubsection*{Index randomly}
In this benchmark, rather than accessing values consecutively, they will be read at random positions. From the algorithmic point of view, the implementation of the operation itself is the same. As indices are picked randomly, it is quite likely that desired values will be located far apart in memory, which causes a cache invalidation. Additionally, results will show whether the performance degenerates with randomness, as it would with linked lists for example. 

The test function contains a loop, which is executed N times. In the loop body, a value is accessed at random index, which is generated within the [0, N) range by using the \emph{\textbf{rand}} crate\footnote{A Rust library for random number generation: \url{https://crates.io/crates/rand}}. According to the \emph{\textbf{rand}} documentation, generated indices are uniformly distributed. The number generator is explicitly seeded to produce the same stream of randomness multiple times.

\subsection{Updating}
There are two dimensions in which the update operation will be evaluated. The first one, similar to the index operation, is the order in which vector values are updated: sequential and random. The second dimension introduces the clone operation, which is used in combination with update to reveal how cloning affects performance. 

Here is the resulting list of benchmarks:
\begin{itemize}
    \item Sequential updates:
    \begin{itemize}
        \item Updating a single vector instance. 
        \item Updating and \emph{cloning} a vector.
    \end{itemize}
    \item Updates at random positions:
    \begin{itemize}
        \item Updating a single vector instance. 
        \item Updating and \emph{cloning} a vector.
    \end{itemize}    
\end{itemize}

The setup routine for all benchmarks is identical. As for the index benchmarks, it generates both balanced and relaxed variants of the tree-based vectors. The type of generated values is an unsigned 64-bit integer. 

The size of the generated vector is determined by the problem size passed as an argument. The problem size domain for benchmarks using clone is [10, 20000], which is smaller compared to the [10, 100000] range, used for benchmarks without clone. This is done to reduce the run time of benchmarks. 

\paragraph*{The cost of naive clone vs path copying}
One of the claimed advantages of \rbvec{} over \stdvec{}, is the cheap clone operation enabled by the path copying \todo{ref:path-copying} algorithm of \rbtree{}. However, the naive copy of \stdvec{} is faster for small-sized vectors due to its simplicity and better locality features. \pvec{} takes the best of both worlds, by using \stdvec{} for the size up to 1024 elements, after which it switches to \rbvec{}. Hence, the objectives of the update benchmarks involving the clone operation are:
\begin{itemize}
    \item Compare performance of naive and path copying algorithms. 
    \item Evaluate the efficiency of dynamic internal representation in \pvec{}.  
\end{itemize}

\paragraph*{The overhead of relaxed nodes in \rrbtree{}}
Relaxed nodes of \rrbtree{} use size tables to keep track of the size of its child nodes. Balanced nodes, on the other hand, do not need them, as the size can be derived from the level of the node. Hence, relaxed nodes are more expensive to clone. Additionally, \rrbtree{} is not perfectly balanced as \rbtree{}, potentially resulting in taller trees. The results of benchmarks will reveal how significant this overhead is in practice. 

\subsubsection*{Update sequentially}
The test function iterates over indices in the [0, N) range, where \emph{N} is the problem size, acquiring a mutable reference to the value at the given position. Once the reference is acquired, it is used to increment the value. 

\subsubsection*{Update randomly}
The test function contains a loop, which is executed \emph{N} times. In the loop body, value is updated by incrementing it, at the index that is randomly generated in the [0, N) range. 

\subsubsection*{Extending benchmarks with the clone operation}
Both benchmarks are extended with the clone operation. The test variant with clone introduces an additional variable for keeping track of the cloned vector. This is done to ensure that at least two vector instances exist at the time when the update is executed. This is necessary because \emph{rc} pointers used to implement \rbtree{}, clone the underlying value on mutation only when the reference count is bigger than one. Thus, by having a cloned instance of vector present in the scope, we enforce the path copying algorithm to be used when updating a vector. 

\subsection{Pushing}
% What
% * Sequential updates to vector:
%  - Updating a value in vector sequentially
%  - Updating a value in cloned vector sequentially
% * Random updates to vector:
%  - Updating a value in vector randomly
%  - Updating a value in cloned vector randomly

% Why
% * Comparison of updating and cloning the vector (path copying algorithm)
% * The difference between updates to balanced / unbalanced vectors
% * The efficiency of dynamic internal representation
%  - Especially in combination with mutation and cloning
%  - The cost of clone for stdvec and rbvec representations of pvec
% * The cost of sequential and random updates to a vector
% * The efficiency of transient updates

% How
% * Four different benchmarks. All of them share the same setup routine.
%  - One that creates a balanced version, and one which creates an unbalanced version. 
% * Sequential benchmark
% * Random benchmark
% * Addition of clone operation and its semantics. 

\subsection{Popping}
\subsection{Concatenating}
\subsection{Slicing}

\section{Parallel vector}
One of the claims is that \rrbvec{} is very efficient when it comes to the split and concatenate operations. The data parallelism frameworks, such as Rayon \todo{ref}, Cilk \todo{ref}, and Scala's parallel collections \todo{ref}, split the work into smaller chunks to ensure good parallelism. Thus, fast split and concat operations are critical for the optimal performance. 

In this section, we will first take a look at how Rayon splits and distributes the work across threads, as well as available configuration parameters. Then, section \todo{} introduces tests for benchmarking the overall performance of persistent and standard vectors:

\begin{itemize}    
    \item Adding elements of two vectors.    
    \item Check if a word is a palindrome.         
\end{itemize}

All the tests will be executed on 1, 2, 4, and 8 threads. 

Unlike the measurements presented from the sequential benchmarks, the parallel ones inlude the run time of both vector operations and Rayon. As the objective is the overall performance comparison, this is considered to be an acceptable tradeoff. 

The results will be used to evaluate the effectivness of the following optimizations:
\begin{itemize}
    \item The effect of relaxed concat and split operations of \rrbvec{} on the overall performance. 
    \item Dynamic internal representation of \pvec{}.     
\end{itemize}

\subsection{Rayon}
The idea of Rayon, a data parallelism library for Rust, is to turn sequential code into parallel with as little work as possible. Loops and iterators are often used to process collections sequentially. Rayon, on the other hand, offers a potentially more efficient alternative to them in the form of parallel iterators. It takes advantage of modern processors, by dividing the work between available cores when it is considered to be beneficial. 

\paragraph*{Parallel iterators}

\begin{figure}[!htbp] 
    \centering

    \begin{minted}{rust}
        // sequential iterator
        vec![1, 2, 3]
            .into_iter()
            .for_each(|x| println!("{}", x));

        // rayon's parallel iterator
        vec![1, 2, 3]
            .into_par_iter()
            .for_each(|x| println!("{}", x));
    \end{minted}

    \caption{Example of using sequential and parallel iterators.}
    \label{fig:par-iter-example}
\end{figure}

The power of iterators in Rust is in the operations which it provides over its elements. Structures which implement those operations are called combinators. They can be chained and the result of execution is passed from one combinator to another. 

Parallel iterators expose similar set of operators, even though not entirely identical. As iterators process values sequentially, there is a set of combinators which expect values to be emitted in particular order. As the parallel iterators are designed to process data in any order, inherentely sequential combinators are simply not applicable. Thus, Rayon might be not suitable for algorithms relying on the sequential order of execution.

Another limitation which parallel iterators impose, is that type of values which it works with have to implement the \emph{Send} trait. It means using non-threadsafe types such as \emph{Rc} in combination with Rayon is prohibited. 

\paragraph*{Work splitting}
\begin{figure}[!htbp]
    \centering

    \begin{tikzpicture}[
        font=\ttfamily,
        array/.style={
            matrix of nodes,
            nodes={draw, minimum size=7mm, fill=green!30},
            column sep=-\pgflinewidth, 
            row sep=0.5mm, 
            nodes in empty cells,        
            row 1 column 1/.style={nodes={draw}}
        }]
                
        \matrix[array] (array) { 
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
        };            
        
        \draw[|-|]([yshift=-4mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {12} ([yshift=-4mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-6.south east);
        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-7.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-3.south east);
        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-4.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-6.south east);
        \draw[loosely dotted]([yshift=-12mm,xshift=1mm]array-1-7.south west) -- ([yshift=-12mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {2} ([yshift=-16mm,xshift=-1mm]array-1-2.south east);
        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-3.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-16mm,xshift=-1mm]array-1-3.south east);
        \draw[loosely dotted]([yshift=-16mm,xshift=1mm]array-1-4.south west) -- ([yshift=-16mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-1.south east);
        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-2.south east);
        \draw[loosely dotted]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- ([yshift=-20mm,xshift=-1mm]array-1-12.south east);    
        
        \draw ([xshift=1mm]array-1-12.east)--++(0:3mm) node [right]{ Vector };
    \end{tikzpicture}

    \caption{Visualization of work splitting in Rayon.}
    \label{fig:rayon-work-splitting}
\end{figure}

One of the Rayon's components, fork/join framework, is responsible for dividing and distributing the work between threads. When parallel iterator receives values from a collection like vector, Rayon attempts to repeatedely divide the work into chunks among threads until the chunk is small enough for a single thread. See an example in figure \ref{fig:rayon-work-splitting}.

As demonstrated in figure \ref{fig:rayon-join}, the work is \emph{potentially} divided between two threads by calling \emph{rayon::join}, which accepts two clojures. Rayon decides whether it is beneficial to parallelize the work, depending on the count of available threads, the split factor and the work load. If the problem is small enough, it is solved sequentially. Otherwise, it is subdivided into smaller parts. When both clojures finish working, the results are combined and returned to the caller. 

The size of the work chunk, or the \emph{split factor}, can be controlled by two operations available for \emph{IndexedParallelIterator}, namely \emph{with\_min\_len} and \emph{with\_max\_len}. The use of combinators which may affect the size of a collection, such as \emph{filter}, returns a \emph{ParallelIterator} which does not support configuration of the \emph{split factor}. 

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        rayon::join(
            || do_something(...),
            || do_something_else(...)
        );
    \end{minted}
    
    \caption{An example of using rayon's join.}
    \label{fig:rayon-join}
\end{figure}

By default, the count of threads allocated by Rayon is equal to the number of cores available in the system. To observe how the thread count affects the performance, Rayon's threadpool will be configured to work with 2, 4, 8, and 16 threads. 

\paragraph*{Load balancing}
In the perfect world, the chunks of work split between threads take the same amount of time to process. In reality, this is often not the case, resulting in some threads idling. In Rayon, each thread has a queue of work attached to it. It keeps processing the queue until it becomes empty. To avoid idling, the thread which has finished processing its queue can steal work from another thread. This technique is known as work-stealing and is used as the main mechanism for work distribution in Rayon. 

\paragraph*{Computation stages}
Computation stages of both "Add elements of two vectors" and "Check if a word is a palindrome" benchmarks, could be described in three steps. 
\begin{itemize}
    \item First, split the work between threads. 
    \item Then process the chunk of work sequentially. 
    \item Finally, combine and return the results.    
\end{itemize}

The final step can be subdivided further:
\begin{itemize}
    \item Collect individual items into a vector using the parallel \emph{Fold} combinator. 
    \item Reduce emitted vectors into a single one using the \emph{Reduce} combinator. 
\end{itemize}

% Explain a little bit further what fold and reduce operators do, and what is the difference between them

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        let result = parallel_iterator
            .fold(Vec::new, |mut vec, x| {
                vec.push(x);
                vec
            })
            .reduce(Vec::new, |mut vec1, mut vec2| {
                vec1.append(&mut vec2);
                vec1
            });
    \end{minted}
    
    \caption{Collecting items of parallel iterator.}
    \label{fig:fold-reduce}
\end{figure}

% You can depict this part as a tree (you already have an example)

%   Thread 1       Thread 2       Thread 3       Thread 4 
% |1| |2| |3|     |4| |5| |6|    |7| |8| |9|   |10| |11| |12|

% increment all values 
% |2| |3| |4|     |5| |6| |7|    |8| |9| |10|  |11| |12| |13|

% fold
%  |2, 3, 4|       |5, 6, 7|      |8, 9, 10|    |11, 12, 13|

% reduce
% |1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12|


\subsection{Benchmarks}
The benchmarks were executed against following vector implementations: Vec, RbVec, RrbVec, and PVec, where RbVec, RrbVec, and PVec are based on the threadsafe reference-counted pointer -- \emph{Arc}. The \imrsvec{} is not included because it does not implement the Rayon's \emph{IntoParallelIterator} trait, which makes its evaluation irrelevant. 

Benchmarks have been parameterized over two dimensions: the vector size and the number of threads. To see whether parallelism is beneficial, each benchmark has an analogous, sequential implementation executed on a single thread. 

\subsubsection*{Sum of elements of two vectors}
Given two equally sized vectors of integers, the test function adds values at the corresponding indices, and returns a new instance of a vector with results. The benchmark is subdivided into three steps:

\begin{enumerate}
    \item Transform each vector into a parallel iterator, and merge them into a single sequence of value pairs. 
    \item Add items of the emitted tuple of two integers into a single value.
    \item Reduce individual sums into a vector of results.
\end{enumerate}

The setup routine prepares two vectors of integers from 0 to N, where N is the problem size. 

\subsubsection*{Check if a word is a palindrome}
The benchmark checks whether a word is a palindrome. As the input, we are using a list of english words consisting of only alphabetic characters. The benchmark consists of two variants:
\begin{itemize}
    \item Annotating every word with a boolean which indicates if the word is a palindrome. 
    \item Collecting palindromes into a new vector. 
\end{itemize}

The computation stages of both tests are very similar code wise, with the difference present in the operators used to process each individual word:

\begin{enumerate}
    \item Transform the given vector of words into a parallel iterator. 
    \item A word processing step for two tests correspondingly:
    \begin{enumerate}
        \item Return a tuple containing the word and the flag which indicates whether the word is a palindrome. 
        \item Filter out all words which are not palindromes. 
    \end{enumerate}
    \item Reduce the results to a new instance of a vector.  
\end{enumerate}

Essentially, the difference comes down to the operators used. For the first test, each word is processed by the \emph{Map} combinator. It takes a string as an argument, and returns a tuple of a string and boolean. 

The second test relies on the \emph{Filter} combinator, which takes a predicate as an argument. The predicate, in this case, is the function checking if the word is a palindrome. 

An important difference in behavior between two tests, is that filter alters the length of the resulting vector, while the map combinator does not. 

Hence, the difference in performance between standard and persistent vector might become more apparent in the second test, as Rayon will not be able to make optimizations based on the assumptions made about the size of resulting vector. 

\paragraph*{The benchmark setup}
The total count of words stored in the file is 370103. As benchmark is parameterized over the size of a vector, which in this case is equal to the number of words, the setup routine caps the count of words by \emph{N}. 

The contents of the file are loaded into memory once, and then before each run they are copied over to a new vector within the setup routine. A vector is later passed to the test routine, which in a turn, converts it to a parallel iterator. 

\section{Presentation of results}
The results show the mean measured time for each function as the input increases. 

\section{Reproducing results}
Benchmarks can be compiled and executed using Rust's package manager named \emph{cargo}. By default, sequential benchmarks are executed against the non-threadsafe variant of the persistent vector. To select the threadsafe variant, user can pass the \emph{arc} feature flag as demonstrated in the figure \ref{fig:sequential-benches}. To run a particular benchmark, specify its name as an argument to cargo. 

\floatstyle{boxed}
\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        # benches of the non-threadsafe implementation
        cargo bench

        # benches of the threadsafe implementation
        cargo bench --features=arc 
    \end{minted}
    
    \caption{Example 1.}
    \label{fig:sequential-benches}
\end{figure}

To execute parallel benchmarks, user needs to pass both the \emph{arc} and \emph{rayon-iter} feature flags: 

\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        cargo bench --features=arc,rayon-iter
    \end{minted}
    
    \caption{Example 2.}
    \label{fig:parallel-benches}
\end{figure}

Criterion is also capable of generating HTML reports with charts, given that gnuplot \footnote{http://www.gnuplot.info/} is installed on the system. Results can be found in the \emph{pvec-rs/target/criterion} directory, and if gnuplot is installed, the report will be available at \emph{pvec-rs/target/criterion/report/index.html}. 

For more information on available options and parameters for Criterion, please consult the library documentation \footnote{https://docs.rs/criterion/0.3.0/criterion/}.
