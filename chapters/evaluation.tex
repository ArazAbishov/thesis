\chapter{Performance evaluation}

In this chapter, I will introduce a methodology for performance evaluation of \rrbvec{}, \pvec{} and their variants, in comparison to implementations from \imrsvec{} and the Rust's standard library. We will look at the details of these three stages:

\begin{itemize}
    \item First, a methodology for collecting reliable measurements. 
    \item Then identifying directions for performance comparisons. 
    \item Finally, defining benchmarks. 
\end{itemize}

To begin with, I will present a notion of benchmarking framework, before diving into details of specific profiling tests. 

\section{Methodology}
Even though \bigo{} \todo{Use a word representation of Big-O} is a useful tool for reasoning about performance in theory, it often is not accurate enough for evaluating real-world performance. First, it disregards constant factors as they are not significant for the growth rate of functions. Second, it does not consider the architecture of CPU and memory \todo{ref}, which indeed influences performance. Furthermore, it also applies to software, such as operating systems, schedulers, virtual machines, et cetera. Hence, often algorithms which are expected to be equally fast based on \bigo{}, may differ substantially in real-world performance. 

This leads us to a need for an experimental performance analysis approach, which would involve executing tests on the actual hardware and software. This, however, introduces another set of unique challenges. For instance, depending on the workload, operating systems may allocate more resources for high demanding tasks, by reducing runtime for others \todo{ref}. Such non-deterministic behavior may lead to profiling results which vary from run to run significantly, which defeats the purpose of having them. 

Benchmarking frameworks were introduced to solve those problems. They are designed to get stable measurements by executing the same test thousands of times. Some of them, such as criterion \todo{ref} for Haskell and Rust, JMH \todo{ref} for Java, and ScalaMeter for Scala, introduce statistical methods for the detection and elimination of exceptionally different runs, known as outliers. 

\subsection{Benchmarking frameworks for Rust}

There are several benchmarking frameworks available for Rust, and unfortunately, none of them have reached a stable release yet. However, some of them are being actively used in the Rust community and proven to produce reliable results.  

In our case, there are several criteria which a good framework has to meet:

\begin{itemize}
    \item Collects multiple samples where each sample consists of multiple runs to ensure consistent results. 
    \item Detection and elimination of outliers.     
    \item A way for setting up a benchmark before each run. 
    \item A way for preventing compiler optimizing benchmark code away.     
\end{itemize}

\subsubsection*{Rust's benchmark tests}
The Rust's testing framework provides an experimental feature which enables developers to write test benchmarks. Those benchmarks are executed thousands of times until results are stabilized. Also, it provides a black-box function \footnote{Black box function contains inline assembly instructions, which compiler cannot make any assumptions about. Hence, it prevents the compiler from optimizing the code which otherwise would be considered "dead" or unused.} which is opaque for the compiler. 

However, it does not detect and eliminate anomalies. It also does not provide APIs for setup routines, which makes it impossible to create benchmarks which rely on certain preconditions.

% ToDo: consider talking about measurement of time for dropping items
\subsubsection*{Criterion for Rust}
Criterion for Rust \todo{ref}, not to be confused with Criterion for Haskell \todo{ref}, is a powerful and statistically rigorous tool for profiling code. It features outlier elimination, setup routines, and is capable to generate graphs provided that gnuplot is installed \todo{ref}. it is available for the stable Rust compiler. Thus, Criterion was chosen as a benchmarking framework for this project. 

\subsection{Execution environment}
All benchmarks were executed on a computer \todo{table with hardware, software} with a quad-core Intel Core i5-6600 processor with hyperthreading, 16GB of DDR4 RAM and 250GB solid-state drive. The operating system of the choice is Ubuntu 18.04 with nightly Rust compiler version \todo{Rust version}.

\subsection{Configuration and input size}
\todo{section}

\section{Benchmarking directions}
% ToDo: you have completely forgotten about branching factor, and it being a standalone feature
% ToDo: you haven't talked about input size and configuration of benchmarks (what will be the input size). You can steal input numbers from scala paper, and then tweak them to run for reasonable amount of time. 

To understand how effective certain optimizations are, we need to evaluate various configurations of the persistent vector. Additionally, implementations from \imrsvec{} and the standard library will be tested too. All vector variants are specified in table \todo{ref}. 

% ToDo: try to name types after the name of types in actual code, otherwise thins will get very confusing very fast. Also, I don't think it is important to show distinction between Rc / Arc flavors of implementations, because it just introduces more confusion. 
\begin{center}
\begin{tabular} { |l| p{10cm} | }
    \hline
    STD Vec & Vec from the Rust's standard library. \\ \hline
    \rbvec{} & \rbtree{} based vector. \\ \hline
    \rrbvec{} & \rrbtree{} based vector. \\ \hline
    \pvec{} & \rrbtree{} based vector with dynamic internal representation. \\ \hline
    \imrsvec{} & \rrbtree{} based vector from third party library \imrsvec{}. \\
    \hline        
\end{tabular}
\end{center}

% \begin{wip}
    % For the purpose of comparison, the measurements are taken against the standard vector, as well as the \rrbtree based persistent vector implementation from the third party library named \emph{im-rs}, which has been introduced at the time of writing this paper. The persistent vector presented in this work, has been evaluated using both non-atomic and atomic reference counted pointers, as well as with and without small sized vector based optimization. 
% \end{wip}

In general, benchmarks described below could be categorized into two groups:

\begin{itemize}
    \item First, serial benchmarks for profiling core operations in a sequential environment. They will be executed both against threadsafe and non-threadsafe variants of the vector. 
    \item Second, concurrent benchmarks which will be executed against only thread-safe variants of the vector. The goal is to check whether there are benefits of using fast split and combine operations of \rrbvec{}.
\end{itemize}


\subsection{RB and RRB Vectors}
As relaxed balanced tree is not perfectly balanced and involves the use of size tables for the radix search, it is expected to be somewhat slower in all core operations. This, however, is not true from the perspective of asymptotic analysis, where constant factors are neglected. The goal of benchmarks, in this case, is to reveal the overhead induced by relaxed nodes. 

Before each benchmark run, an instance of \rrbvec{} will be prepared by concatenating pseudo-random small vectors together. The amount of relaxed nodes is in part affected by the size of the vector. Both threadsafe and non-threadsafe variants will be compared. 

% ToDo: how to consistenly generate RRB Vectors using pseudo randomly sized small vectors? Can you get access to benchmarks from paper anywhere? 
% ToDo: add list of benchmarks which will be evaluating different core operations. 

\subsection{Unique access or transience}
While \rrbvec{} performs very well as a persistent data structure, it is not very optimal when properties of persistence are not required. An example is a function which creates and returns an instance of \rrbvec{}, where all versions except the returned one are disregarded.

Luckily, the persistent vector presented in this project takes advantage of Rust's compiler capabilities of tracking object aliasing. Thus, it avoids redundant copying on mutation if the given object is uniquely accessed. This behavior is somewhat similar to transience in the Clojure's persistent vector, but not entirely identical \todo{see reference}. 

In Rust, non-transient, persistent behavior can be enforced by cloning the object before performing a mutation. The goal is to measure the overhead of using clone operation in the persistent vector. 

\subsection{Dynamic internal representation}
As one of the suggested optimizations in \todo{ref to scala paper}, a standard vector can be used to improve the performance of small-sized \rrbvec{}. The size recommended for using the standard vector representation is 4096. However, dynamically switching representation during runtime comes at a cost, which potentially may offset the benefits. 

The purpose of profiling this optimization is to understand whether it improves performance in practice, and in which use cases. The range of problem sizes will include small values. 

\todo{describe an experiment; i.e. how many threads, split factor, problem sizes, etc.}
\todo{how number of threads is configured in rayon: 1, 2, 4, 8, 16, 32 and 64}
\todo{which experiments will you run?}

\subsection{Memory overhead}
\todo{tbd}

\subsection{Rc vs Arc}
Since atomic reference-counted pointers are claimed to introduce additional overhead in comparison to their non-threadsafe counterpart, the goal is to check how significant is the difference. 

As a part of all sequential benchmarks, both threadsafe and non-threadsafe variants will be evaluated. Rc based flavor will not be present in parallel benchmarks, as the Rust's compiler prevents non-threadsafe types being used across threads. 

\section{Parallel vector}
One of the claims is that \rrbvec{} is very efficient when it comes to split and concatenate operations. The data parallelism frameworks, such as Rayon \todo{ref}, Cilk \todo{ref}, and Scala's parallel collections \todo{ref}, split the work into smaller chunks to ensure good parallelism. Thus, fast split and concat operations are critical for optimal performance. 

In this section, we will first take a look at how Rayon splits and distributes the work across threads, as well as available configuration parameters. Then, section \todo{} introduces tests for benchmarking the overall performance of persistent and standard vectors:

\begin{itemize}    
    \item Processing a vector of integers.    
    \item Check if a word is a palindrome.         
\end{itemize}

All the tests will be executed on 1, 2, 4, 8, and 16 threads. 

Unlike the measurements presented from the sequential benchmarks, the parallel ones inlude the run time of both vector operations and Rayon. As the objective is the overall performance comparison, this is considered to be an acceptable tradeoff. 

The results will be used to evaluate the effectivness of following optimizations:
\begin{itemize}
    \item The effect of relaxed concat and split operations of \rrbvec{} on the overall performance. 
    \item Dynamic internal representation of \pvec{}.     
\end{itemize}

\subsection{Rayon}
The idea of Rayon, a data parallelism library for Rust, is to turn sequential code into parallel with as little work as possible. Loops and iterators are often used to process collections sequentially. Rayon on the other hand, offers a potentially more efficient alternative to them in the form of parallel iterators. It takes advantage of modern processors, by dividing the work between available cores when it is considered to be beneficial. 

\paragraph*{Parallel iterators}

\begin{figure}[!htbp] 
    \centering

    \begin{minted}{rust}
        // sequential iterator
        vec![1, 2, 3]
            .into_iter()
            .for_each(|x| println!("{}", x));

        // rayon's parallel iterator
        vec![1, 2, 3]
            .into_par_iter()
            .for_each(|x| println!("{}", x));
    \end{minted}

    \caption{Example of using sequential and parallel iterators.}
    \label{fig:par-iter-example}
\end{figure}

The power of iterators in Rust is in the operations which it provides over its elements. Structures which implement those operations are called combinators. They can be chained and the result of execution is passed from one combinator to another. 

Parallel iterators expose similar set of operators, even though not entirely identical. As iterators process values sequentially, there is a set of combinators which expect values to be emitted in particular order. As the parallel iterators are designed to process data in any order, inherentely sequential combinators are simply not applicable. Thus, Rayon might be not suitable for algorithms relying on the sequential order of execution.

Another limitation which parallel iterators impose, is that type of values which it works with have to implement the \emph{Send} trait. It means using non-threadsafe types such as \emph{Rc} in combination with Rayon is prohibited. 

\paragraph*{Work splitting}
\begin{figure}[!htbp]
    \centering

    \begin{tikzpicture}[
        font=\ttfamily,
        array/.style={
            matrix of nodes,
            nodes={draw, minimum size=7mm, fill=green!30},
            column sep=-\pgflinewidth, 
            row sep=0.5mm, 
            nodes in empty cells,        
            row 1 column 1/.style={nodes={draw}}
        }]
                
        \matrix[array] (array) { 
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
        };            
        
        \draw[|-|]([yshift=-4mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {12} ([yshift=-4mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-6.south east);
        \draw[|-|]([yshift=-8mm,xshift=1mm]array-1-7.south west) -- node[above,font=\tiny,outer sep=0mm] {6} ([yshift=-8mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-3.south east);
        \draw[|-|]([yshift=-12mm,xshift=1mm]array-1-4.south west) -- node[above,font=\tiny,outer sep=0mm] {3} ([yshift=-12mm,xshift=-1mm]array-1-6.south east);
        \draw[loosely dotted]([yshift=-12mm,xshift=1mm]array-1-7.south west) -- ([yshift=-12mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {2} ([yshift=-16mm,xshift=-1mm]array-1-2.south east);
        \draw[|-|]([yshift=-16mm,xshift=1mm]array-1-3.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-16mm,xshift=-1mm]array-1-3.south east);
        \draw[loosely dotted]([yshift=-16mm,xshift=1mm]array-1-4.south west) -- ([yshift=-16mm,xshift=-1mm]array-1-12.south east);

        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-1.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-1.south east);
        \draw[|-|]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- node[above,font=\tiny,outer sep=0mm] {1} ([yshift=-20mm,xshift=-1mm]array-1-2.south east);
        \draw[loosely dotted]([yshift=-20mm,xshift=1mm]array-1-2.south west) -- ([yshift=-20mm,xshift=-1mm]array-1-12.south east);    
        
        \draw ([xshift=1mm]array-1-12.east)--++(0:3mm) node [right]{ Vector };
    \end{tikzpicture}

    \caption{Visualization of work splitting in Rayon.}
    \label{fig:rayon-work-splitting}
\end{figure}

One of the Rayon's components, fork/join framework, is responsible for dividing and distributing the work between threads. When parallel iterator receives values from a collection like vector, Rayon attempts to repeatedely divide the work into chunks among threads until the chunk is small enough for a single thread. See an example in figure \ref{fig:rayon-work-splitting}.

As demonstrated in figure \ref{fig:rayon-join}, the work is \emph{potentially} divided between two threads by calling \emph{rayon::join}, which accepts two clojures. Rayon decides whether it is beneficial to parallelize the work, depending on the count of available threads, the split factor and the work load. If the problem is small enough, it is solved sequentially. Otherwise, it is subdivided into smaller parts. When both clojures finish working, the results are combined and returned to the caller. 

The size of the work chunk, or the \emph{split factor}, can be controlled by two operations available for \emph{IndexedParallelIterator}, namely \emph{with\_min\_len} and \emph{with\_max\_len}. The use of combinators which may affect the size of a collection, such as \emph{filter}, returns a \emph{ParallelIterator} which does not support configuration of the \emph{split factor}. 

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        rayon::join(
            || do_something(...),
            || do_something_else(...)
        );
    \end{minted}
    
    \caption{An example of using rayon's join.}
    \label{fig:rayon-join}
\end{figure}

By default, the count of threads allocated by Rayon is equal to the number of cores available in the system. To observe how the thread count affects the performance, Rayon's threadpool will be configured to work with 2, 4, 8, and 16 threads. 

\paragraph*{Load balancing}
In perfect world the chunks of work split between threads take the same amount of time to process. In reality this is often not the case, resulting in some threads idling. In Rayon, each thread has a queue of work attached to it. It keeps processing the queue until it becomes empty. In order to avoid idling, the thread which has finished processing its queue can steal work from another thread. This technique is know as work stealing and is used as the main mechnasim for work distribution in Rayon. 

\paragraph*{Computation stages}
Computation stages of both "Processing a vector of integers" and "Check if a word is a palindrome" benchmarks, could be described in three steps. 
\begin{itemize}
    \item First, split the work between threads. 
    \item Then process the chunk of work sequentially. 
    \item Finally, combine and return the results:    
\end{itemize}

The final step can be subdivided further:
\begin{itemize}
    \item Collect individual items into a vector using the parallel \emph{Fold} combinator. 
    \item Reduce emitted vectors into a single one using the \emph{Reduce} combinator. 
\end{itemize}

% Explain a little bit further what fold and reduce operators do, and what is the difference between them

\begin{figure}[!htbp]
    \centering

    \begin{minted}{rust}
        let result = parallel_iterator
            .fold(Vec::new, |mut vec, x| {
                vec.push(x);
                vec
            })
            .reduce(Vec::new, |mut vec1, mut vec2| {
                vec1.append(&mut vec2);
                vec1
            });
    \end{minted}
    
    \caption{Collecting items of parallel iterator.}
    \label{fig:fold-reduce}
\end{figure}

% You can depict this part as a tree (you already have an example)

%   Thread 1       Thread 2       Thread 3       Thread 4 
% |1| |2| |3|     |4| |5| |6|    |7| |8| |9|   |10| |11| |12|

% increment all values 
% |2| |3| |4|     |5| |6| |7|    |8| |9| |10|  |11| |12| |13|

% fold
%  |2, 3, 4|       |5, 6, 7|      |8, 9, 10|    |11, 12, 13|

% reduce
% |1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12|


\subsection{Benchmarks}
The benchmarks were executed against following vector implementations: Vec, RbVec, RrbVec, and PVec, where RbVec, RrbVec, and PVec are based on the threadsafe reference-counted pointer -- \emph{Arc}. The \imrsvec{} is not included because it does not implement the Rayon's \emph{IntoParallelIterator} trait, which makes its evaluation irrelevant. 

Benchmarks have been parameterized over two dimensions: the vector size and the number of threads. To see whether parallelism is beneficial, each benchmark has an analogous, sequential implementation executed on a single thread. 

\subsubsection*{Processing a vector of integers}
This benchmark is subdivided into two separate tests: 
\begin{itemize}
    \item Incrementing each item of a vector.
    \item Filtering out odd integers. 
\end{itemize}

The setup routine of this benchmark is identical for both tests, which generates a vector of [0, N] integers, where \emph{N} is the problem size. As soon as processing is complete, parallel iterator is reduced to a vector. 

\paragraph*{Check if a word is a palindrome}
For each word in the given file, check if it is a palindrome. Words are provided in a text file separated by spaces. The result of execution is a list of pairs, where the first item is the word and the second is a flag indicating whether it is a palindrome or not. 

% how the file is loaded and what is the setup routine
The contents of the file are loaded into memory once, and then before each run they are copied over to a new vector within the setup routine. A vector is later passed to the test routine, which in a turn, converts it to a parallel iterator. 

% what is the range of problem sizes 
The total count of words in the file is 370103. Even though all words are available, the setup routine only returns N words, where N is the problem size. The range of problems for this benchmark is [10000, 370103]. 

\section{Presentation of results}
The results show the mean measured time for each function as the input increases. 

\section{Reproducing results}
To execute sequential benchmarks do this. In order to execute sequential benchmarks using \emph{Arc} use this. If you want to run parallel benchmarks, do this and this. Criterion can as well generate reports with charts using gnuplot, but you need to make sure that it is installed on the system. You need to be located in the root directory of the project. 

Executing sequential benchmarks:
\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        # benches of the non-threadsafe implementation
        cargo bench

        # benches of the threadsafe implementation
        cargo bench --features=arc
    \end{minted}
    
    \caption{Example 1.}
    \label{fig:sequential-benches}
\end{figure}

Executing parallel benchmarks:
\begin{figure}[!htbp]
    \centering

    \begin{minted}{bash}
        cargo bench --features=arc,rayon-iter
    \end{minted}
    
    \caption{Example 2.}
    \label{fig:parallel-benches}
\end{figure}

You can find the results in the next directory: project/target/criterion/. If you had gnuplot installed, then the report with generated charts can be found here: project/target/criterion/report/index.html. 